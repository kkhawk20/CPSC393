{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Homework 2\"\n",
        "author: \"Kelsey Hawkins\"\n",
        "format: pdf\n",
        "---"
      ],
      "id": "4acfadeb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "ASL is a prominent language in the American deaf community. With this in mind, I have a dataset of images containing the ASL dictionary, excluding the letters that need motion. The dataset was already flattened into greyscale values, ready for model input with a few preprocessing changes. This model will help translate ASL images to text for those who do not understand it, or want to learn it. \n",
        "\n",
        "# Analysis \n",
        "The first exploratory item I did was visualizing a few images from the dataset. Not only did I want to ensure the data accuracy, but I wanted to see what resolution I was working with. It was hard to do any summary statistics or heatmaps, etc. due to the data being flattened images so no EDA would have been useful in this case.\n",
        "\n",
        "However, the images have a 28 x 28 x 1 size, a very low resolution image of hands depicting each letter of the american alphabet. Since the images have been flattened into columns of greyscale values, there will be 784 columns, and all values except for the predictor variable were scaled between 0-1. The predictor variable is a single number that denotes a letter of the alphabet, excluding J and Z which require motion. \n"
      ],
      "id": "13a16d90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Random Images from dataset\n",
        "#| fig-cap: Random Images from Dataset\n",
        "#| echo: false\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import keras as kb\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from plotnine import *\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "train_df = pd.read_csv(\"/Users/kelseyhawkins/Desktop/CPSC_Courses/CPSC_393/sign_mnist_train.csv\")\n",
        "test_df = pd.read_csv(\"/Users/kelseyhawkins/Desktop/CPSC_Courses/CPSC_393/sign_mnist_test.csv\")\n",
        "\n",
        "# Separating X and Y\n",
        "y_train = train_df['label']\n",
        "y_test = test_df['label']\n",
        "\n",
        "del train_df['label']\n",
        "del test_df['label']\n",
        "\n",
        "# rescale data to be 0-1 instead of 0-255\n",
        "trainX = train_df.astype(\"float32\") / 255.0\n",
        "testX = test_df.astype(\"float32\") / 255.0\n",
        "\n",
        "# change the labels to be in the correct format\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(y_train)\n",
        "testY = lb.transform(y_test)\n",
        "\n",
        "trainX.head()\n",
        "trainX.shape\n",
        "\n",
        "print(trainX.shape,\n",
        "trainY.shape)\n",
        "\n",
        "print(testX.shape,\n",
        "testY.shape)\n",
        "\n",
        "# Visualize some images!!!\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# I used different names cuz i wanted to reshape them without\n",
        "# Changing the original data put into the model :)\n",
        "x_train = train_df.values\n",
        "x_test = test_df.values\n",
        "x_train_vis = x_train.reshape(-1,28,28,1)\n",
        "x_test = x_test.reshape(-1,28,28,1)\n",
        "f, ax = plt.subplots(2,5)\n",
        "f.set_size_inches(10, 10)\n",
        "k = 0\n",
        "for i in range(2):\n",
        "    for j in range(5):\n",
        "        ax[i,j].imshow(x_train_vis[k].reshape(28, 28) , cmap = \"gray\")\n",
        "        k += 1\n",
        "    #plt.tight_layout()    \n"
      ],
      "id": "Random-Images-from-dataset",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Methods\n",
        "The first model I created is a Deep Feed-Forward Neural Network. I started with a simple model, built in class that had about 6 hidden layers. I then expanded it by adding more dropout, dense, and batch normalization layers. Each dense layer has an activation function of reLU, and a kernel regularizer of L2 on two early dense layers. The last dense layer outputs 24 nodes with a softmax activation function for predicted probabilities for each letter. When I compiled my model, I tested multiple optimizers including SGD, Adam, and RMSprop. The model included early stopping to prevent overfitting. \n",
        "\n",
        "The second model I created is a Random Forest Classifier. I created a simple one to get a general sense of the model outputs and comparison to the DNN. Then, I built a tuned RFC by utilizing the Randomized Search CV module. The parameters I tuned were the number of estimators, max depth, min samples split and leaf. The scorers were a combination of accuracy and F1 for a combination between all three classification metrics. \n",
        "\n",
        "# Results\n",
        "With my two models, it is proven that deep learning is not needed. In the deep neural network, no matter how many layers I added or removed, or tuning I did, the metrics did not significantly improve. Both the accuracy and F1 scores were extremely low and the model failed to fit to the data well. Especially since I could not use convolutional layers, the neural network failed to find features well. The training and testing scores were not far from eachother, so no underfitting or overfitting was present. However, the testing accuracy was 34%, and the F1 score was also 34%. \n"
      ],
      "id": "d0808fba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Model_output_DNN_graph\n",
        "#| fig-cap: Model validation and test over epochs\n",
        "#| echo: false\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import keras as kb\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from plotnine import *\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "# build structure of the model!! DNN\n",
        "# The images are naturally 28*28*1 shape, so input will be 784 (columns)\n",
        "model = kb.Sequential([\n",
        "    kb.layers.Dense(500, input_shape = [784]), #input\n",
        "    kb.layers.Dense(300, activation='relu', kernel_regularizer = \"l2\"),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dropout(0.2),\n",
        "    kb.layers.Dense(150, activation='relu', kernel_regularizer = \"l1\"),\n",
        "    kb.layers.Dropout(0.2),\n",
        "    kb.layers.Dense(90, activation='relu'),\n",
        "    kb.layers.Dropout(0.2),\n",
        "    kb.layers.Dense(50, activation='relu'),\n",
        "    kb.layers.BatchNormalization(),\n",
        "    kb.layers.Dense(24, activation = \"softmax\") #output\n",
        "])\n",
        "\n",
        "# compile model, commenting out the optimizer I dont want (from me testing them out hehe)\n",
        "# Did the best with SGD!!\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "early = [kb.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)]\n",
        "\n",
        "history = model.fit(trainX, trainY, epochs = 100, validation_data=(testX, testY), callbacks = early, verbose = 0)\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "id": "Model_output_DNN_graph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Model_output_DNN_Images\n",
        "#| fig-cap: DNN Predictions over Test Images\n",
        "#| echo: false\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import keras as kb\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from plotnine import *\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "key = {0:'a', 1:'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f',\n",
        "       6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k',\n",
        "       11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p',\n",
        "       16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u',\n",
        "       21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z'\n",
        "       }\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predicting on test dataset\n",
        "test_pred = model.predict(testX)\n",
        "test_pred_labels = np.argmax(test_pred, axis=1)\n",
        "\n",
        "# Actual labels in numeric form\n",
        "true_labels = np.argmax(testY, axis=1)\n",
        "\n",
        "# Mapping numeric labels back to letters using 'key' dictionary\n",
        "mapped_true_labels = [key[label] for label in true_labels]\n",
        "mapped_pred_labels = [key[label] for label in test_pred_labels]\n",
        "\n",
        "# Select a few random images from the test set to display\n",
        "num_images = 5\n",
        "random_indices = np.random.choice(testX.shape[0], num_images, replace=False)\n",
        "\n",
        "# Setup for a 1x5 grid\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(15, 15))\n",
        "\n",
        "for ax, idx in zip(axes.flat, random_indices):\n",
        "    # Reshape the image for display\n",
        "    image = testX.iloc[idx].values.reshape(28, 28)  # Reshape back to 28x28 for display\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.set_title(f'Actual: {mapped_true_labels[idx]}\\nPredicted: {mapped_pred_labels[idx]}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "Model_output_DNN_Images",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the random forest classifier model, the metrics were quite different than the DNN. This further proves deep learning is not necessary in this context as the comparison of scores were multitudes different. The random forest classifier outputted very high metrics once it was tuned. The tuner found the best number of estimators to be 300, the min samples split of 2, and min samples leaf to be 1 and max depth of 20. The training and testing accuracy showed signs of overfitting as the training accuracy was 100% and the testing accuracy was 83%. In the images below, we can see the predictions were valid for all five images, as opposed to the predictions made by the DNN. \n"
      ],
      "id": "0383e794"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Model_output_RFC_Images\n",
        "#| fig-cap: RFC Predictions over Test Images\n",
        "#| echo: false\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import keras as kb\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from plotnine import *\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "# A more complex RFC with randomizedSearchCV for parameter tuning! \n",
        "# This took 20 minutes on a GPU\n",
        "\n",
        "# Assuming trainY might be one-hot encoded, convert if necessary:\n",
        "if len(trainY.shape) > 1:  # trainY is one-hot encoded\n",
        "    y_train_labels = np.argmax(trainY, axis=1)\n",
        "else:\n",
        "    y_train_labels = trainY  # trainY is already in label format\n",
        "\n",
        "# parameter distributions to search\n",
        "param_distributions = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize the classifier\n",
        "rf = RandomForestClassifier(random_state=123)\n",
        "\n",
        "# scorers for model performance\n",
        "scorers = {\n",
        "    'accuracy_score': make_scorer(accuracy_score),\n",
        "    'f1_score': make_scorer(f1_score, average='micro')\n",
        "}\n",
        "\n",
        "# Setup RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions, n_iter=10, scoring=scorers,\n",
        "                                    refit='f1_score', return_train_score=True, cv=2, verbose=1, random_state=123)\n",
        "\n",
        "# Fit the random search to the data\n",
        "random_search.fit(trainX, y_train_labels)\n",
        "\n",
        "# Best parameters and scores\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# Predict on the test data\n",
        "test_preds = best_rf.predict(testX)\n",
        "train_preds = best_rf.predict(trainX)\n",
        "\n",
        "# testY might be one-hot encoded, convert if necessary:\n",
        "if len(testY.shape) > 1:  # testY is one-hot encoded\n",
        "    y_test_labels = np.argmax(testY, axis=1)\n",
        "else:\n",
        "    y_test_labels = testY  # testY is already in label format\n",
        "\n",
        "# Quarto does not like these for some reason... \n",
        "# # Evaluate the best model\n",
        "# accuracy_test = accuracy_score(y_test_labels, test_preds)\n",
        "# f1_score_test = f1_score(y_test_labels, test_preds, average='micro')\n",
        "\n",
        "# accuracy_train = accuracy_score(y_train_labels, train_preds)\n",
        "# f1_score_train = f1_score(y_train_labels, train_preds, average='micro')\n",
        "\n",
        "# print(f\"Testing Accuracy: {accuracy_test:.4f}, F1 Score (Micro): {f1_score_test:.4f}\")\n",
        "# print(f\"Training Accuracy: {accuracy_train:.4f}, F1 Score (Micro): {f1_score_train:.4f}\")\n",
        "\n",
        "# Predicting on images!!\n",
        "\n",
        "test_pred_labels = best_rf.predict(testX)\n",
        "\n",
        "# Convert testY from one-hot encoding to class labels if necessary\n",
        "if len(testY.shape) > 1:\n",
        "    true_labels = np.argmax(testY, axis=1)\n",
        "else:\n",
        "    true_labels = testY  # Assuming testY is already class labels\n",
        "\n",
        "# Mapping numeric labels back to letters using the 'key' dictionary provided\n",
        "mapped_true_labels = [key[label] for label in true_labels]\n",
        "mapped_pred_labels = [key[label] for label in test_pred_labels]\n",
        "\n",
        "# Select a few random images from the test set to display\n",
        "num_images = 5\n",
        "random_indices = np.random.choice(testX.shape[0], num_images, replace=False)\n",
        "\n",
        "# Setup for a 1x5 grid\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(15, 3))  # Adjust figsize for better display\n",
        "\n",
        "for ax, idx in zip(axes.flat, random_indices):\n",
        "    # Reshape the image for display\n",
        "    # Ensure testX is properly accessed depending on its format (DataFrame or ndarray)\n",
        "    if hasattr(testX, 'iloc'):  # testX is a DataFrame\n",
        "        image = testX.iloc[idx].values.reshape(28, 28)\n",
        "    else:  # testX is an ndarray\n",
        "        image = testX[idx].reshape(28, 28)\n",
        "\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.set_title(f'Actual: {mapped_true_labels[idx]}\\nPredicted: {mapped_pred_labels[idx]}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "Model_output_RFC_Images",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reflection\n",
        "In this assignment, I did not come across many struggles or problems with the code or analysis. However I did refer back to the math of the models to justify my answers and think about why the models performed the way they did. It was interesting to be able to quantify them with a simple classification problem and think about how each model handles class imbalances in the slightest way, by about 20%. In the future, I would approach it the same way as I did here, as it seemed to work well. \n"
      ],
      "id": "03e55cfa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}